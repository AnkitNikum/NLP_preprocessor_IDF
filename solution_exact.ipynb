{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('news.csv')\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "sw_nltk = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from num2words import num2words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def stopword_fun(x):\n",
    "    if len(x['content'].split('.')) == 1:\n",
    "        text = x['content'].replace('  ','. ')\n",
    "    else:\n",
    "        text = x['content']\n",
    "    words = [word for word in text.split() if word.lower() not in sw_nltk]\n",
    "    text = \" \".join(words)\n",
    "    text = \"\".join([i for i in text if i not in '''!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~]'''])\n",
    "    words = [WordNetLemmatizer().lemmatize(word) for word in text.split(' ')]\n",
    "    text = ''\n",
    "    for word in words:\n",
    "        \n",
    "        if word.isnumeric() :\n",
    "            text  = text + \" \" +num2words(float(word))\n",
    "        elif word[:-1].isnumeric():\n",
    "            text  = text + \" \" +num2words(float(word[:-1])) + '.'\n",
    "        elif len(word) >2:\n",
    "            text =  text + \" \" + word\n",
    "    words = [word for word in text.split() if word.lower() not in sw_nltk]    \n",
    "    text = \" \".join(words)\n",
    "    text = \"\".join([i for i in text if i not in '''!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~]'''])\n",
    "    text = text.lower()\n",
    "    return text\n",
    "def stopword_fun_title(x):\n",
    "    if len(x['title'].split('.')) == 1:\n",
    "        text = x['title'].replace('  ','. ')\n",
    "    else:\n",
    "        text = x['title']\n",
    "    words = [word for word in text.split() if word.lower() not in sw_nltk]\n",
    "    text = \" \".join(words)\n",
    "    text = \"\".join([i for i in text if i not in '''!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~]'''])\n",
    "    words = [WordNetLemmatizer().lemmatize(word) for word in text.split(' ')]\n",
    "    text = ''\n",
    "    for word in words:\n",
    "        \n",
    "        if word.isnumeric() :\n",
    "            text  = text + \" \" +num2words(float(word))\n",
    "        elif word[:-1].isnumeric():\n",
    "            text  = text + \" \" +num2words(float(word[:-1])) + '.'\n",
    "        elif len(word) >2:\n",
    "            text =  text + \" \" + word\n",
    "    words = [word for word in text.split() if word.lower() not in sw_nltk]    \n",
    "    text = \" \".join(words)\n",
    "    text = \"\".join([i for i in text if i not in '''!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~]'''])\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "content = train.apply(stopword_fun,axis=1)\n",
    "\n",
    "title = train.apply(stopword_fun_title,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644 644\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize(content1):\n",
    "    sent_dict = {}\n",
    "    sentence = content1.split('. ')\n",
    "    for j,i in enumerate(sentence):\n",
    "        sent_dict[j] = i.split()\n",
    "    return sent_dict\n",
    "processed_text = {}\n",
    "\n",
    "\n",
    "print(len(content),len(title))\n",
    "for j,i in enumerate(content):\n",
    "\n",
    "    processed_text[j] = tokenize(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = { i:{} for i in range(len(content))}\n",
    "from collections import Counter\n",
    "for i in range(len(content)):\n",
    "  for j in processed_text[i].keys():\n",
    "    tokens = processed_text[i][j]\n",
    "    for w in tokens:\n",
    "        if w in DF[i].keys():\n",
    "            DF[i][w].add(j)\n",
    "        else:\n",
    "            DF[i][w] = {j}\n",
    "   \n",
    "\n",
    "for i in DF:\n",
    "   for j in DF[i]:\n",
    "     DF[i][j] = len(DF[i][j])\n",
    "\n",
    "def doc_freq(word,j):\n",
    "    c = 0\n",
    "    if word in DF[j].keys():\n",
    "        c = DF[j][word]\n",
    "    else:\n",
    "        pass\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 0\n",
    "\n",
    "\n",
    "N = len(content)\n",
    "tf_idf = { i:{} for i in range(len(content))}\n",
    "for i in range(N):\n",
    "  n = len (processed_text[i])\n",
    "  tf_idf[doc] = { k:{} for k in range(n)}\n",
    "  for j in processed_text[i].keys():\n",
    "    \n",
    "    tokens = processed_text[i][j]\n",
    "    \n",
    "    counter = Counter(tokens )\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df_val = doc_freq(token,i)\n",
    "        idf = np.log((n+1)/(df_val+1))\n",
    "        \n",
    "        tf_idf[doc][j][token] = tf*idf\n",
    "\n",
    "  doc += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4011973816621555 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17901038850853448"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0\n",
    "average_tfidf_sent = []\n",
    "max = 0\n",
    "minimum = 0\n",
    "for i in tf_idf:\n",
    "    ad = []\n",
    "    \n",
    "    for j in tf_idf[i]:\n",
    "        n = len(tf_idf[i][j])\n",
    "        average_tfidf = sum(tf_idf[i][j].values())/n\n",
    "        if average_tfidf > max:\n",
    "            max = average_tfidf\n",
    "        if average_tfidf < minimum:\n",
    "            minimum = average_tfidf\n",
    "        ad.append(average_tfidf)\n",
    "    average_tfidf_sent.append(ad)\n",
    "print(max,minimum)  \n",
    "threshold = minimum +  (max - minimum)/19\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq2(word):\n",
    "    c = 0\n",
    "    if word in DF.keys():\n",
    "        c = DF[word]\n",
    "    else:\n",
    "        pass\n",
    "    return c\n",
    "def remove_sentence(x,threshold):\n",
    "    text = x['content_mod']\n",
    "    processed_text = tokenize(text)\n",
    "    DF={}\n",
    "    for j in processed_text.keys():\n",
    "        tokens = processed_text[j]\n",
    "        for w in tokens:\n",
    "            if w in DF.keys():\n",
    "                DF[w].add(j)\n",
    "            else:\n",
    "                DF[w] = {j}\n",
    "       \n",
    "    for i in DF:\n",
    "         DF[i] = len(DF[i])\n",
    "    doc = 0\n",
    "    n = len(processed_text)\n",
    "    tf_idf = {i:{} for i in range(n)}\n",
    "    for i in range(n):\n",
    "        \n",
    "        tokens = processed_text[i]\n",
    "        \n",
    "        counter = Counter(tokens )\n",
    "        words_count = len(tokens)\n",
    "        \n",
    "        for token in np.unique(tokens):\n",
    "            \n",
    "            tf = counter[token]/words_count\n",
    "            df_val = doc_freq2(token)\n",
    "            idf = np.log((n+1)/(df_val+1))\n",
    "            \n",
    "            tf_idf[i][token] = tf*idf\n",
    "    \n",
    "        doc += 1\n",
    "    remove = []\n",
    "    retain = []\n",
    "    for i in tf_idf:\n",
    "        n = len(tf_idf[i])\n",
    "        if sum(tf_idf[i].values())/n < threshold:\n",
    "          \n",
    "            remove.append(i)\n",
    "        else:\n",
    "           \n",
    "            retain.append(i)\n",
    "    remove_text = ''\n",
    "    retain_text = ''\n",
    "    for i in remove:\n",
    "        remove_text +=  ' '.join(processed_text[i])+ '. ' \n",
    "    for i in retain:\n",
    "        retain_text +=  ' '.join(processed_text[i]) + '. ' \n",
    "    x['new_content'] = retain_text\n",
    "    x['removed_lines'] = remove_text\n",
    "    return x\n",
    "\n",
    "test['content_mod'] = test.apply(stopword_fun,axis=1)\n",
    "test = test.apply(remove_sentence,args=[threshold],axis=1)\n",
    "test[['content','removed_lines','new_content']].to_csv('result.csv')\n",
    "\n",
    "            \n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
